---
title: "[논문리뷰] Deep Residual Learning for Image Recognition (ResNet)"
last_modified_at: 2025-02-10
categories:
  - 논문리뷰
tags:
  - Computer Vision
  - AI
  - Image Classification
  - ResNet Model

excerpt: "ResNet 논문 리뷰"
use_math: true
classes: wide
math_engine: katex
---

> CVPR 2016. [[논문 링크](https://arxiv.org/abs/1512.03385)] [[Github](https://github.com/KaimingHe/deep-residual-networks)]  
> 저자: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (Microsoft Research)  
> 출판일: 2015년 12월 10일

## 소개

ResNet(Residual Network)은 깊은 신경망의 학습을 용이하게 하기 위해 **Residual Learning** 개념을 도입한 모델이다. 기존의 CNN이 네트워크가 깊어질수록 **Gradient Vanishing** 문제가 발생하는 반면, ResNet은 **Skip Connection**을 통해 이를 해결하였다.

이 논문에서 제안된 주요 개념은 다음과 같다:
1. **Residual Learning**: 신경망의 각 블록이 직접 학습하는 것이 아니라, 변화를 학습하도록 설계하여 학습을 안정화한다.
2. **Deep Networks Training**: 152개의 깊은 층에서도 성능이 감소하지 않고 효과적으로 학습할 수 있도록 설계됨.
3. **Skip Connection**: Identity mapping을 통해 Backpropagation을 용이하게 만들어 학습 속도를 향상시킨다.

## ResNet 구조

ResNet은 깊은 신경망을 효과적으로 학습할 수 있도록 설계된 **Residual Block**을 사용한다.

### 1. Residual Learning
기존 네트워크에서 입력을 그대로 출력으로 전달하는 것이 아니라, **Residual Mapping**을 학습한다:

$$
H(x) = F(x) + x
$$

여기서, \( H(x) \)는 최종 출력, \( F(x) \)는 학습할 Residual Function, \( x \)는 입력이다.

이 방식은 학습할 것이 많을수록 학습이 어려워지는 일반적인 CNN 구조와 달리, 네트워크가 학습해야 할 변화를 단순화하여 최적화 과정이 더 효율적으로 진행된다.

### 2. Bottleneck 구조
ResNet은 연산량을 줄이면서도 성능을 유지하기 위해 Bottleneck 구조를 사용한다.

$$
F(x) = W_3 \sigma ( W_2 \sigma ( W_1 x ) )
$$

여기서, \( W_1, W_2, W_3 \)는 \( 1 \times 1, 3 \times 3, 1 \times 1 \) Convolution 연산을 의미하며, 이를 통해 계산량을 줄이고 성능을 유지할 수 있다.

### 3. ResNet의 블록 구성

ResNet은 다양한 깊이의 네트워크를 구축할 수 있도록 설계되었으며, 대표적인 모델은 다음과 같다:
- **ResNet-18**: 18개 레이어
- **ResNet-34**: 34개 레이어
- **ResNet-50**: 50개 레이어 (Bottleneck 구조 사용)
- **ResNet-101**: 101개 레이어
- **ResNet-152**: 152개 레이어

## 실험 및 결과

ResNet은 ImageNet 데이터셋에서 SOTA(State-of-the-Art) 성능을 달성하였다. 주요 실험 결과는 다음과 같다:

- **ImageNet 분류 성능**: 152-layer ResNet이 기존 모델보다 낮은 Top-5 Error Rate를 기록함.
- **Object Detection**: Faster R-CNN과 결합하여 객체 검출에서도 뛰어난 성능을 보임.
- **Transfer Learning**: 다양한 컴퓨터 비전 작업에 쉽게 적용 가능함.

### 주요 성능 비교

ResNet-152는 기존 VGG-16보다 8배 깊은 구조임에도 불구하고 계산량은 절반으로 감소하였으며, 성능은 더욱 향상되었다.

![ResNet 성능 비교](/assets/img/resnet/resnet-table1.webp)

## 결론

ResNet은 신경망을 깊게 쌓을 때 발생하는 **Gradient Vanishing** 문제를 Skip Connection을 통해 해결한 획기적인 모델이다. 주요한 특징은 다음과 같다:
1. **Residual Learning 도입**: 기존 CNN과 달리 네트워크가 변화를 학습하도록 유도하여 최적화가 용이함.
2. **매우 깊은 네트워크 구현 가능**: 152-layer까지 확장하면서도 성능이 저하되지 않음.
3. **다양한 컴퓨터 비전 태스크에서 활용 가능**: 이미지 분류, 객체 검출, 세분화 등에 적용 가능함.

ResNet은 이후 DenseNet, EfficientNet, Vision Transformer 등 다양한 모델에 영향을 미쳤으며, 현재까지도 널리 사용되고 있다.

